# 监督学习

## 简单案例

假设我们有一个数据集，给出了俄勒冈州勃兰特地区47个房子的住房面积和价格：

![住房面积和价格对应表](住房面积.PNG)

我们可以用散点图来描述上面的这些数据：

![房价模型散点图](房价模型散点图.PNG)

当我们想根据这个图片了解某个任意房屋面积下的房子价格的时候，依靠这个图片是办不到的。因此我们需要建立一个函数来解决这个问题。可以使用我们在高中时期学过的回归**函数**对这个模型进行求解。

解题步骤：

- 令 $x^{(i)}$ 代表 "$input$" 变量（在这个例子中的住房面积），输入变量也可以称作输入$特征$

- 令 $y^{(i)}$ 代表我们需要去预测的 "$output$" 或者$目标变量$（房屋价格）。一个数对 $(x^{(i)},y^{(i)})$ 被称作是一个训练样本
-  ${(x^{(i)},y^{(i)});i=1,...,m}$  被称作一个$训练集$。这个例子中 $m=47$

【**特别注意**】角标 "(i)" 仅仅是一个训练集的索引，和指数并没有任何关系。

我们将使用 $\mathcal{X}$ 代表输入值空间，用 $\mathcal{Y}$ 表示输出值空间。在这个例子中，$\mathcal{X}=\mathcal{Y}=\mathbb{R}$。

为了更加正式的描述**监督学习的定义**，我们可以这样说：

> 给定一个目标，一个训练集，学习一个函数$h:\mathcal{X \mapsto Y}$。

如果满足这个映射关系，我们说$h(x)$是与之对应的$y$的一个很好的预测器。由于历史原因，函数$h$被叫做是假设。整个过程的流程如下所示：

![监督学习流程图](监督学习流程图.PNG)

- 当目标变量是连续的时候（比如在房价模型中）我们把它归类到为回归问题。
- 当目标变量$y$不连续的时候（比如给出房屋面积，让我们去预测这个房子是一个普通的小区住房还是别墅）我们把它归类到分类问题。

## 线性回归

为了让我们的房价模型更加有趣，我们考虑丰富一下我们的数据集，增加一个卧室数量的特征：

![woshishuliang](woshishuliang.PNG)

现在我们的输入变了变成了二维的空间向量（因为有两个特征：面积，卧室数量）。另外，我们用符号来表示该模型的时候，$x_1^{(i)}$ 表示第 $i$ 个房屋的面积，$x_2^{(i)}$ 表示第 $i$ 个房子的卧室数量（通常来讲，设计一个学习算法，选择多少个输入特征完全由你自己决定，所以如果你是外地人，想要在勃兰特购买房子的话，你可以收集其他的房屋信息，比如是否有一个炉灶，卫生间的数目等。我们后面也会讲到多个特征的选取，但是现在仅考虑给出的两个特征）。

为了能够让计算机执行监督学习算法，我们需要决定如何去表示函数（假设）$h$ 。作为一个最初的选择，我们决定估计 $y$ 是一个关于 $x$ 的线性函数：
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2
$$
其中，$\theta_i$ 称为**参数**（也叫做**权重**），它用来界定描述 $\mathcal{X}$ 映射到 $\mathcal{Y}$ 的线性函数的空间。当没有什么疑惑的时候，我们把 $\theta$ 写成映射关系的角标并记作 $h_\theta(x)$ ，简写为 $h_\theta(x)$ 。为了简化我们的式子，令 $x_0=1$（这一项表示**截距**），简写如下：
$$
h(x)=\sum\limits_{i=0}^n\theta_ix_i=x^T\theta
$$
等号的右侧，$\theta$ 和 $x$ 都是向量，$n$ 表示输入变量的个数（而不是 $x_0$ 个数），为了便于理解，输入矩阵可能长下面这个样子：
$$
\begin{matrix}
x_{11} & ... & x_{1n} \\\\
. &  & . \\\\
. & . & . \\\\
. &   & . \\\\
x_{m1} & ... & x_{mn}
\end{matrix}
$$
而 $\theta$ 长这个样子：
$$
\begin{matrix}
\theta_{1}\\\\
\theta_{2}\\\\
. \\\\
. \\\\
\theta_{n} 
\end{matrix}
$$
现在，给定训练集 $x$ ，我们如何选取和学习参数 $\theta$ ，这就是监督学习要做到事情了。

一个合理的解决方案就是令 $h(x)$ 不断的接近其真值 $y$ ，至少我们在训练的例子中是这样做的。这句话好说，人也可以理解，但是机器不能，现在就可以公式化，让机器可以理解：定义一个函数，它用来衡量对每个 $\theta$ 取值 $h(x^{(i)})$ 和 $y^{(i)}$ 的误差是多少，也就是下面的**损失函数**：
$$
J(\theta)=\frac 1 2 \sum\limits_{i=1}^m{(h_\theta(x^{(i)})-y^{(i)})^2}
$$
如果你之前接触过线性回归，你可能会意识到这个损失函数和最小二乘法相似，并有此产生了一个普通的最小二乘回归模型。无论你之前是否了解过，现在我们继续，最终我们会展示这是非常广泛的算法族中的一个特殊情况。

### 最小均方（LMS）算法

为了能够得到使 $J(\theta)$ 最小的 $\theta$ ，我们对上面的算法需要先做一个假想的 $\theta$ （也就是说我们猜想，$\theta$ 在这个取值下，$J(\theta)$ 可以取到最小值，当然假想可能不正确，然后我们重复的对 $\theta$ 进行取值，令 $J(\theta)$ 不断的变小，直到我们不论对 $\theta$ 取何值，都能得到一个相同的 $J(\theta)$ ）。特别的，我们考虑使用**梯度下降**算法，这个算法是选取了一个初始的 $\theta$ 然后重复的更新目标值：

![gxgs](gxgs.png)

（如果你的计算机是多核的话，可以同时的执行对于 $j=0,...n$ 所有取值下的更新）。

其中，$\alpha$ 称为**学习率**，这是一个非常自然的算法，每次按照梯度**下降最快的方向**前进一小步（对 $J$ 求梯度）。

进一步解释说明这个算法，我们需要计算出在等号的右侧的偏导数。我们首先计算只有一个训练样本 $(x,y)$ 的情况，在这种情况下我们可以忽略掉 $J$ 的求和符号，然后进行推广：

$$
\begin{align\*}
\frac{\partial}{\partial\theta_j}J(\theta) &=\frac{\partial}{\partial\theta_j}\frac 1 2 {(h_\theta(x)-y)^2}\\\\
&=2·\frac 1 2 {(h_\theta(x)-y)}·\frac{\partial}{\partial\theta_j}(h_\theta(x)-y)\\\\
&={(h_\theta(x)-y)}·\frac{\partial}{\partial\theta_j}(\sum\limits_{i=1}^n \theta_ix_i-y)\\\\
&=(h_\theta(x)-y)x_j
\end{align\*}
$$

对于整个训练样本，给出的更新规则如下：

$$
\theta_j:=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$

这个规则被称为是**LMS**更新规则（least mean squares）有时也被称为是 **Widrow-Hoff** 学习规则。这个规则有几个直观、自然的属性。举例来说，更新的快慢取决于**误差**项 $h_\theta(x^{(i)})-y^{(i)}$ ；因此，如果我们一开始对 $\theta$ 的取值误差很小的话，我们发现几乎不需要更改参数 $\theta$，相反，如果误差较大，那么参数会发生较大的变化来使预测值 $h_\theta(x^{(i)})$ 更加的接近真实值 $y^{(i)}$。

我们已经对一个训练样本进行了扩展。有两种方法可以对这个算法进行修改，使其可以训练多个样本。首先用下面的算法代替之前的算法：

![congfu](congfu.png)

这个阅读器可以很容易的确定求和项的数目。这是对原始的损失函数应用了一个简单的梯度下降。这个方法每进行一步就会遍历所有的样本，这个算法被称为**批量梯度下降**。补充一下，梯度下降容易陷入局部最优解，而我们对这个模型简单化，我们选择的是只有一个全局最优解，没有局部最优，因此梯度下降算法总是能够找到一个最小值。（如果有多个局部最优解，需要考虑优化算法）事实上，$J$ 是一个凸二次函数。这里有一个关于运行梯度下降算法求二次函数最小值的例子。

![tuercihanshu](tuercihanshu.PNG)

上面的椭圆表示的是二次函数的轮廓图（曲线图）。蓝色的曲线表示的是梯度下降的路径，选取的初始值是（48,30）。图中的 $x$ （蓝色的短直线）标记了 $\theta$ 的连续下降路径。

当我们运行批量梯度下降寻找合适的 $\theta$ 的时候（之前的房价模型），我们可以得到一个一个房屋的预测价格函数（关于住房面积），最终求得 $\theta_0=71.27, \theta_1=0.1345$。画出 $h_\theta(x)$ 关于 $x$（住房面积） 的函数图像，如下：

![zhufang](zhufang.PNG)

如果加入卧室数量这个输入特征，可以得到 $\theta_0=89.60, \theta_1=0.1392, \theta_2=-8.738$ 。

上面的计算结果是根据批量梯度下降算法得到的。另一种方法也可以取得很好的效果，其数学描述如下：

![loopfor](loopfor.png)

在这个算法中，重复的运算更新规则，每次训练一个样本，然后根据这一个训练样本的误差按照梯度的方向更新一次参数，这个算法叫做**随机梯度下降法**（也叫做**梯度上升法**）。批量梯度下降发法每次更新参数都需要遍历所有的训练样本，如果训练集的样本数目 $m$ 很大，随机梯度下降法的优势就体现出来了，然后根据训练样本继续更新参数。通常，随机梯度下降可能得到的 $\theta$ 并不是全局最优解，而是和全局最优解很相近的一个数，随机梯度下降法的收敛速度要比批量梯度下降法快很多。虽然随机梯度下降法无法得到一个全局最优解，但是在实际应用中，这个近似值已经足够了。正是由于这个原因，在庞大的数据集下，随机梯度下降法可以表现出比批量梯度下降法更优秀的效果。

### 正规方程（The normal equations）

梯度下降法给出了一个最小化 $J$ 的方式。现在讨论第二种方式，这次计算用直接的方式来进行计算最小化，不采用迭代算法。我们将显式的取 $\theta_j$ 的导数来最小化 $J$ ，将 $\theta_j$ 设置为 $0$  。为了能够不浪费大量的纸张，只是在纸上写矩阵的求导运算，我们介绍几个符号来代替这些求导工作。

#### 矩阵求导

对一个函数 $f:\mathbb{R}^{m×n}\mapsto\mathbb{R}$ ，其表示一个 $m×n$ 的矩阵到实数的计算过程，对 $f$ 关于 $A$ 的求导，定义如下：
$$
\nabla_Af(A)=\left[\begin{array}{cc}
\frac{\partial f}{\partial A_{11}} & ... & \frac{\partial f}{\partial A_{1n}} \\\\ 
. &  & . \\\\
. & . & . \\\\
. &   & . \\\\
\frac{\partial f}{\partial A_{m1}} & ... & \frac{\partial f}{\partial A_{mn}} 
\end{array}\right]
$$
因此，$\nabla_Af(A)$ 自身是一个 $m×n$ 的矩阵，第 $(i,j)$ 个元素是 $\partial{f}/\partial{A_{ij}}$ 。举例来说，假设 $A=\left[\begin{array}{cc}A_{11}&A_{12} \\\\A_{21}&A_{22}\end{array}\right]$ 是一个 $2×2$ 的矩阵，函数 $f:\mathbb{R}^{2×2}\mapsto\mathbb{R}$ 如下：
$$
f(A)=\frac{3}{2}A_{11}+5A_{12}^2+A_{21}A_{22}
$$
其中，$A_{ij}$ 表示矩阵的第 $(i,j)$  个元素，我们有下面的推导：
$$
\nabla_Af(A)=\left[\begin{array}{cc}\frac{3}{2}&10A_{12} \\\\A_{22}&A_{21}\end{array}\right]
$$
另外，介绍一下矩阵的**迹**（trace）运算符号，记作 “tr”。对于一个 $n×n$ 的方阵 $A$，它的迹被定义为对角线元素的和：
$$
trA=\sum\limits_{i=1}^nA_{ii}
$$
如果 $a$ 是一个实数（i.e.，$a$ 是一个 $1*1$ 的矩阵）,那么 $tr \space a= a$ 。（如果你之前没有见过这个操作符号，你可以把 $A$ 的迹想成 $tr(A)$，或者 “trace” 函数在矩阵 $A$ 上的一个作用。通常情况下，我们不写括号。）

对于两个矩阵 $A$ 和 $B$ 来说，迹运算下面的几个运算性质，$AB$ 是一个方阵，我们可以得到 $trAB=trBA$（你可以自己验算）根据这个式子可以推导出下面的结论：
$$
trABC = trCAB = trBCA \\\\
trABCD = trDABC = trCDAB = trBCDA
$$
下面的性质也是很好证明的，其中 $A$ 和 $B$ 是方阵，$a$ 是一个实数。
$$
trA = trA^T\\\\
tr(A + B) = trA + trB\\\\
tr\ aA = a\ trA
$$
现在对下面的公理不加以证明（这些矩阵的求导运算在后面的推导过程中会用到）。等式（4）仅适用于非奇异性矩阵 $A$，其中 $|A|$ 表示矩阵 $A$ 对应行列式的值。
$$
\begin{alignat}{2}
\nabla_AtrAB &= B^T \\\\
\nabla_{A^T} f(A) &= (\nabla_Af(A))^T \\\\
\nabla_AtrABA^TC &= CAB + C^TAB^T \\\\
\nabla A|A| &= |A|(A^{−1})^T \\\\
\end{alignat}
$$
进一步说明，假设我们有一个固定的矩阵 $B \in R^{n×m}$ 。我们可以根据 $f(A)=trAB$ 定义一个函数 $f:\mathbb{R}^{m×n}\mapsto\mathbb{R}$ 。为了使这个定义起作用，因为如果 $A ∈ R^{m×n}$ ，那么 $AB$ 就是一个方阵，然后我们可以应用上面的迹运算；因此，$f$ 实际上执行了一个从 $\mathbb{R}^{m×n}$ 到 $\mathbb{R}$ 的运算。我们可以用我们的矩阵求导的定义来计算 $\nabla_Af(A)$ ，它将由一个 $m×n$ 的矩阵构成。等式 （1）表示了这个矩阵的第 $(i,j)$ 个元素等于 $B^T$ 的第 $(i,j)$ 个元素，或者，记作 $B_{ji}$。

等式（1）到（3）的证明是非常简单的，留给读者自行证明。等式（4）可以利用逆矩阵的伴随矩阵得到。

### 最小二乘回归

具备了矩阵求导知识，我们继续寻找使得 $J(\theta)$ 最小化的 $\theta$ 的显式解。我们重新用矩阵向量的形式标记 $J$ 。

给定一个训练集，定义**设计矩阵** $X$ 为一个 $m×n$ 的矩阵（实际上是一个 $m×(n+1)$ 矩阵，如果包括截距项的话），它每行都是一个输入数据的训练样本：
$$
X=\left[\begin{array}{cc}
— & (X^{(1)^T}) & — \\\\ 
— & (X^{(2)^T}) & — \\\\
 & . &  \\\\
 & . &  \\\\
 & . &  \\\\
— & (X^{(m)^T}) & —
\end{array}\right]
$$

同样的，我们令 $\vec{y}$ 为一个 $m$ 维的矩阵，它包含了训练样本的目标值：

$$
\vec{y}=\left[\begin{array}{cc}
y^{(1)} \\\\ 
y^{(2)} \\\\
 . \\\\
 . \\\\
 . \\\\
y^{(m)}
\end{array}\right]
$$

现在，因为 $h_\theta(x^{(i)})=(x^{(i)})^T \theta$ ，我们可以很容易的推出：
$$
\begin{align\*}
X\theta-\vec{y} &= \left[\begin{array}{cc}
(x^{(1)})^T\theta \\\\
 · \\\\
 · \\\\
 · \\\\
(x^{(m)})^T\theta
\end{array}    
\right]
-
\left[\begin{array}{cc}
y^{(1)} \\\\
 · \\\\
 · \\\\
 · \\\\
y^{(m)}
\end{array}    
\right] \\\\
& = \left[\begin{array}{cc}
h_\theta(x^{(1)})-y^{(1)} \\\\
 · \\\\
 · \\\\
 · \\\\
h_\theta(x^{(m)})-y^{(m)}
\end{array}    
\right]
\end{align\*}
$$


因此，应用公理，对于一个向量 $\mathit{z}$ ，有 $\mathit{z}^T\mathit{z}=\sum\limits_{i}\mathit{z}_i^2$ ，则对上面应用：
$$
\frac 1 2 (X\theta-\vec{y})^T(X\theta-\vec{y})=\frac 1 2 \sum\limits{i=1}^m( h\theta(x^{(i)})-y^{(i)})^2
$$


最后，为了最小化 $J$ ，我们来对它关于 $\theta$ 进行求导。联合等式（2）和（3），得到：
$$
\begin{align}
\nabla_AtrABA^TC=B^TA^AC^T+BA^TC
\end{align}
$$
因此，
$$
\begin{align\*}
\nabla_AJ(\theta) &= \nabla_\theta\frac 1 2 (X\theta-\vec{y})^T(X\theta-\vec{y}) \\\\
& =\frac 1 2\nabla_\theta (\theta X^TX\theta-\theta^T X^T\vec{y}-\vec{y}^TX\theta+\vec{y}^T\vec{y}) \\\\
& =\frac 1 2\nabla_\theta tr(\theta X^TX\theta-\theta^T X^T\vec{y}-\vec{y}^TX\theta+\vec{y}^T\vec{y})\\\\
& = \frac 1 2\nabla_\theta (tr\theta^TX^TX\theta−2tr\vec{y}^TX\theta) \\\\
& = \frac 1 2 (X^TX\theta+X^TX\theta-2X^T\vec{y}) \\\\
& = X^TX\theta-X^T\vec{y}
\end{align\*}
$$
上面的推导过程的第三步，使用了“实数的迹仍然是实数”这个规则，在第四步，用到了公式 $trA=trA^T$，第五步用到了公式（5）$A=\theta, B=B^T=X^TX$，并且 $C=I$，$I$ 是一个单位阵，为了最小化 $J$ ，我们设他的这个导数的值为 $0$，得到等式：
$$
X^TX\theta=X^T\vec{y}
$$
因此，使 $J$ 最小的 $\theta$ 等于：
$$
\theta=(X^TX)^{-1}X^T\vec{y}
$$

#### 概率解释

当我们解决回归问题时，为什么要选择用线性回归，而且为什么要用最小二乘法损失函数 $J$。在这部分，我们将给出一系列的概率假设，在这个假设下，我们可以从一个很自然的算法中得出最小二乘回归函数。

假设目标变量和输入通过下面的等式建立联系：
$$
y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}
$$
其中，$\epsilon^{(i)}$ 是一个误差项，它包含了未进行建模的因素（比如，和预测房屋价格有直接关系的有很多很多特征，但是我们却没有考虑在内）和随机噪声。让我们进一步假设 $\epsilon^{(i)}$ 服从 IID 分布（独立同同分配），这个分布为均值为零，方差为 $\sigma^2$ 高斯分布（也叫做正态分布）。这个假设可以记作： $\epsilon^{(i)}\sim\mathcal{N}(0,\sigma^2)$ 。例如，$\epsilon^{(i)}$ 的密度可以用下式表示：
$$
p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})
$$
进一步推导：
$$
p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
$$
 "$p(y^{(i)}|x^{(i)};\theta)$" 指出了这个分布是给定 $x^{(i)}$ 来预测 $y^{(i)}$（$\theta$ 是给定好的）。需要注意的是 $\theta$ 不是条件 "$p(y^{(i)}|x^{(i)},\theta)$" ，因为 \theta 不是一个随机变量。我们也可以把 $y^{(i)}$ 的分布写作 $y^{(i)}|x^{(i)}\sim\mathcal{N}(\theta^Tx^{(i)},\sigma^2)$ 。

给定 $X$ （设计矩阵，它包含了所有的 $x^{(i)}$）和 $\theta$ ，那么 $y^{(i)}$ 服从什么分布？相应的 $\vec{y}$ 的概率可以由 $p(\vec{y}|X;\theta)$ 确定。这个量通常被看作是 $\vec{y}$ 的函数（或者 $X$）参数固定为 $\theta$ 。当我们想明确的将这个量看作是关于 $\theta$ 的一个函数的时候，可以把它称作是**似然函数**：
$$
L(\theta)=L(\theta;X,\vec{y})=p(\vec{y}|X;\theta)
$$
注意到 $\epsilon^{(i)}$ 是一个独立性假设（暗示：$y^{(i)}$ 和 $x^{(i)}$ 也是相互独立的），因此 $L(\theta)$ 可以写作：
$$
\begin{align\*}
L(\theta) &=\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\\\
&= \prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{align\*}
$$
现在，给出的这个概率模型把 $y^{(i)}$ 和 $x^{(i)}$ 联系到了一起，如何用合适的方法来选择我们对参数 $\theta$ 最好的猜想呢？原则就是**极大似然估计**所说，我们选择的 $\theta$ 应该使得概率 $p$ 越大越好。比如，我们选择的 $\theta$ 使得 $L(\theta)$ 取得最大值。

除了最大化 $L(\theta)$ ，我们也可以最大化能够使 $L(\theta)$ 增大的其他函数。特别的，为了最大化 $L(\theta)$ ，我们可以间接的最大化它的**对数似然函数 **$\ell(\theta)$ :
$$
\begin{align\*}
\ell(\theta) &= \mathbb{log}L(\theta) \\\\
&= \mathbb{log}\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \\\\
&= \sum_{i=1}^m\mathbb{log}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \\\\
&= m\mathbb{\mathbb{log}}\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma^2}·\frac{1}{2}\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2
\end{align\*}
$$

因此，最大化 $\ell(\theta)$ 和最大化下式是一个效果：
$$
\frac{1}{2}\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2
$$
这个式子其实就是前面的 $J(\theta)$ -- 我们一开始提到的最小二乘损失函数。

总结：在对数据做好了概率性假设的前提下，最小二乘回归和我们要寻找的使得似然函数最大的参数 $\theta$ 存在着对应关系。因此，这是一个可以在很自然的数学推导下可以得出的结论，在这个过程总我们仅仅是为了做极大似然估计。（注意到概率性假设并不是求解最小二乘的必要步骤，仅仅是因为它工作的很好。实际上，还有其他的方法可以取得类似的效果。）

同时，在之前的结论中，我们最后选择一个 $\theta$ 并不依赖于 $\sigma^2$ ,事实上，即使我们不知道 $\sigma^2$ 的取值，同样可以得到相同的结论。我们将在后面讨论指数族和广义线性模型的时候使用这个结论。

## 局部加权回归

考虑到根据 $x\in\mathbb{R}$ 来预测 $y$。最左边的图形展示的是根据数据集来拟合一条曲线 $y=\theta_0+\theta_1x$。我们发现数据并不是仅仅依赖于一条直线，所以这个拟合效果并不好。

![1537427521242](1.1.2.4.01.PNG)

因此，我们添加一个特征 $x^2$，然后我们用拟合曲线 $y=\theta_0+\theta_1x+\theta_2x^2$，然后我们获得了一个稍微好一些的拟合曲线（中间的图）。于是我们猜想是否给出越多的特征值，拟合的效果就越好呢？答案是：不是的。如果我们添加太多的特征值，会出现一个危险的情况，那就是上图最右面的图片了。它拟合了一个五次函数 $y=\sum_{j=0}^5\theta_jx^j$。我们发现虽然拟合曲线穿过了所有的点，但是这并不是我们想要的曲线，因为从这个曲线上我们无法发现什么规律，对于预测房屋价格没有什么帮助。在不定义这些符号都是什么意思的情况下，我们说最左边的例子是 **欠拟合**——因为它并没有捕捉到所有的特征；最右边的例子是**过拟合**（在这个课程的后面，我们将讨论学习理论，在那里，我们会规范化这些概念，更加严谨的定义和评估这些假设是好是坏）



正如前面讨论的，可以看上面的例子，选择合适的特征对于提高一个学习算法的性能是十分重要的。（当我们讨论模型选择的时候，我们也会讲到让算法自动的选择数据集）。在这部分，我们简单的讨论局部加权回归（LWR）算法，假设我们有足够多的训练集，这可以让结果更加客观。操作很简单，因此你也有机会在作业中去探索LWR算法的特性。



在原来的线性回归算法中，预测在一个给定点的值 $x$ （i.e. 评估 $h(x)$），我们要做

1. 寻找合适的 $\theta$ 最小化 $\sum_i(y^{(i)}-\theta^Tx^{(i)})^2$
2. 输出 $\theta^Tx$

相反的，在局部加权回归算法中，要做：

1. 寻找合适的 $\theta$ 最小化 $\sum_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$
2. 输出 $\theta^Tx$

其中，$w^{(i)}$ 是非负的**权重值**，很明显，如果 $w^{(i)}$ 对于一个确定的 $i$ 很大的话，当我们选择 $\theta$ 时，我们将尽量使 $(y^{(i)}-\theta^Tx^{(i)})^2$ 很小。如果 $w^{(i)}$ 很小的话，那么误差项 $(y^{(i)}-\theta^Tx^{(i)})^2$ 在拟合的过程中就几乎可以忽略了。



对于权重，一个公认的标准选择是：
$$
w^{(i)}=exp(-\frac{(x^{(i)}-x)^2}{2\tau^2})
$$

注意到，权重取决于特定的 $x$ (也就是我们想要预测的点)。除此之外，如果 $|x^{(i)}-x|$ 很小，那么权重 $w^{(i)}$ 就接近 1；如果 $|x^{(i)}-x|$ 很大，那木权重就很小。因此，$\theta$ 被选择一个给予很高的权重对于训练样本中的靠近预测点 $x$ 的样本点。（我们也注意到，权重的这个等式和高斯分布十分相似，但是 $w^{(i)}$ 却和高斯分布没有任何关系，另外，$w^{(i)}$并不是随机变量，正太分布或者其他的）参数 $\tau$ 决定了训练样本从 $x^{(i)}$ 到 目标值 $x$ 的权重下降的快慢；$\tau$ 被叫做是**带宽**参数，你也会从作业中亲自感受到这个参数。



局部加权回归是第一个我们接触的**无参数**的算法。线性回归算法（不加权重）是一个**有参数**的学习算法，因为它有一个固定的无穷多数量的参数（$\theta_i$），我们利用这些来拟合数据。一旦我们求出了 $\theta_i$ 并且保存了，我们就不再需要原始的训练集就可以预测其他变量。相反的，如果使用局部加权回归算法预测数据，我们需要保存所有的训练集。无参数（牵强的说）指的是为了代表假设 $h$ ,我们保存的东西的数量随着我们训练集的大小线性增长